{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementing a Simple MLP for Disease Prediction Using NHANES Dataset**\n",
        "\n",
        "## **Introduction**\n",
        "\n",
        "In this assignment, you will preprocess the National Health and Nutrition Examination Survey (NHANES) dataset and implement a simple Multi-Layer Perceptron (MLP) from scratch using NumPy. The goal is to predict the occurrence of either Cancer or Liver disease based on various health and demographic features.\n",
        "\n",
        "## **Dataset Overview**\n",
        "\n",
        "The NHANES dataset provides a comprehensive set of health and nutritional data collected through interviews and medical examinations. It includes information on demographics, dietary habits, medical history, and laboratory results.\n",
        "\n",
        "### **Data Components:**\n",
        "\n",
        "- **Demographics:** Information about age, gender, ethnicity, etc.\n",
        "- **Diet:** Dietary intake and related questions.\n",
        "- **Examination:** Medical and physiological measurements.\n",
        "- **Labs:** Laboratory test results.\n",
        "\n",
        "You can refer to the following links for detailed explanations of each component:\n",
        "\n",
        "- [Demographics](https://wwwn.cdc.gov/Nchs/Nhanes/Search/DataPage.aspx?Component=demographic&CycleBeginYear=2013)\n",
        "- [Diet](https://wwwn.cdc.gov/Nchs/Nhanes/Search/DataPage.aspx?Component=diet&CycleBeginYear=2013)\n",
        "- [Examination](https://wwwn.cdc.gov/Nchs/Nhanes/Search/DataPage.aspx?Component=examination&CycleBeginYear=2013)\n",
        "- [Labs](https://wwwn.cdc.gov/Nchs/Nhanes/Search/DataPage.aspx?Component=labs&CycleBeginYear=2013)\n",
        "\n",
        "## **Tasks Overview**\n",
        "\n",
        "1. **Data Preprocessing**\n",
        "2. **Model Training**\n",
        "3. **Implementing MLP from Scratch**\n",
        "4. **Training and Evaluating the Model**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Data Preprocessing**\n",
        "\n",
        "Data preprocessing is a critical step in any machine learning project. It involves cleaning and transforming raw data to make it suitable for analysis and modeling.\n",
        "\n",
        "### **1.1. Import Necessary Libraries**\n"
      ],
      "metadata": {
        "id": "sYX7Q173PhW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy import stats\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ],
      "metadata": {
        "id": "JcSJYNJlRiPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2. Load the Dataset**\n",
        "\n",
        "You will be provided with four CSV files corresponding to different components of the NHANES dataset. Your first task is to load these datasets."
      ],
      "metadata": {
        "id": "bqq6UZzgRmKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ##TODO: Load the datasets into pandas DataFrames\n",
        "df1 = ##TODO\n",
        "df2 = ##TODO\n",
        "df3 = ##TODO\n",
        "df4 = ##TODO"
      ],
      "metadata": {
        "id": "ggSc9vjpRoMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- `df1` to `df4` correspond to Demographics, Diet, Examination, and Labs datasets respectively.\n",
        "- Use `pd.read_csv()` to load each CSV file.\n",
        "\n",
        "### **1.3. Merge the Datasets**\n",
        "\n",
        "After loading the datasets, you need to merge them into a single DataFrame for easier analysis."
      ],
      "metadata": {
        "id": "4le4_pFXRrxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ##TODO: Merge the DataFrames on a common column, typically 'SEQN'\n",
        "merged_df = ##TODO"
      ],
      "metadata": {
        "id": "0A9VzNWHRt5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- The `SEQN` column is a unique identifier that can be used to merge the datasets.\n",
        "- Ensure that the merge is performed correctly to include all relevant data.\n",
        "\n",
        "### **1.4. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Perform EDA to understand the structure and statistical properties of the dataset."
      ],
      "metadata": {
        "id": "LXHvoZ5nRwMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the shape of the dataset\n",
        "merged_df.##TODO\n",
        "\n",
        "# Display basic statistics\n",
        "merged_df.##TODO\n",
        "\n",
        "# Display the first few rows\n",
        "merged_df.##TODO"
      ],
      "metadata": {
        "id": "uczsrRYlRy7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Use `.shape` to get the dimensions.\n",
        "- Use `.describe()` for statistical summary.\n",
        "- Use `.head()` to view the first few rows.\n",
        "\n",
        "### **1.5. Handle Columns with Fixed Values**\n",
        "\n",
        "Remove columns that have only one unique value as they do not contribute to the predictive power."
      ],
      "metadata": {
        "id": "IopEmrXPR7xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ##TODO: Identify and drop columns with only one unique value\n",
        "dropcols = ##TODO\n",
        "merged_df.drop(dropcols, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "yeR0kjWjSAjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Columns with a single unique value are not useful for modeling.\n",
        "- Identify such columns using `.nunique()` and drop them.\n",
        "\n",
        "### **1.6. Handle Missing Values and Inconsistencies**\n",
        "\n",
        "Replace specific codes (`7` and `9`) with `NaN` and handle missing values appropriately.\n"
      ],
      "metadata": {
        "id": "VEjOdJqDSM6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to replace specific patterns with NaN\n",
        "def replace_with_nan(element):\n",
        "    try:\n",
        "        if re.match(pattern, str(int(element))):\n",
        "            return np.nan\n",
        "        return element\n",
        "    except:\n",
        "        return element\n",
        "\n",
        "# Replace '7's with NaN\n",
        "pattern = r'^7+$'\n",
        "merged_df = merged_df.##TODO\n",
        "\n",
        "# Replace '9's with NaN\n",
        "pattern = r'^9+$'\n",
        "merged_df = merged_df.##TODO\n",
        "\n",
        "# Drop columns with more than 30% missing values\n",
        "merged_df.##TODO"
      ],
      "metadata": {
        "id": "z8sOzZzYSQj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Use regex patterns to identify and replace values.\n",
        "- Drop columns that have a high percentage of missing values to maintain data quality.\n",
        "\n",
        "### **1.7. Segregate Numerical and Categorical Variables**\n",
        "\n",
        "Differentiate between numerical and categorical variables for appropriate preprocessing."
      ],
      "metadata": {
        "id": "hXUGyyTOSS-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numerical variables\n",
        "numerical = ##TODO\n",
        "\n",
        "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
        "print('The numerical variables are :', numerical)\n",
        "merged_df[numerical].head()\n",
        "\n",
        "# Identify categorical variables\n",
        "categorical = ##TODO\n",
        "merged_df[categorical].head()"
      ],
      "metadata": {
        "id": "pATTxiJVSV6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Numerical variables are typically of type `float64` or `int64`.\n",
        "- Categorical variables might be of type `object` or `int64` with limited unique values.\n",
        "\n",
        "### **1.8. Handle Outliers in Numerical Variables**\n",
        "\n",
        "Detect and visualize outliers in numerical features."
      ],
      "metadata": {
        "id": "_C92FuJoSZHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in numerical variables\n",
        "merged_df[numerical].isnull().sum()\n",
        "\n",
        "# View summary statistics\n",
        "print(round(merged_df[numerical].describe(), 2))\n",
        "\n",
        "# Function to find columns with outliers\n",
        "def find_outlier_cols(df):\n",
        "    outlier_cols = []\n",
        "    for col in df.columns:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        if df[(df[col] < lower_bound) | (df[col] > upper_bound)].any(axis=None):\n",
        "            outlier_cols.append(col)\n",
        "    return outlier_cols\n",
        "\n",
        "outlier_cols = find_outlier_cols(merged_df[numerical])\n",
        "print(outlier_cols)\n",
        "len(outlier_cols), len(numerical)"
      ],
      "metadata": {
        "id": "ryClLQmwScRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization:**"
      ],
      "metadata": {
        "id": "OpZb45loSd-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "fig = merged_df.boxplot(column='DMDBORN4')\n",
        "fig.set_title('')\n",
        "fig.set_ylabel('DMDBORN4')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "fig = merged_df.boxplot(column='DMDCITZN')\n",
        "fig.set_title('')\n",
        "fig.set_ylabel('DMDCITZN')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "fig = merged_df.boxplot(column='FIALANG')\n",
        "fig.set_title('')\n",
        "fig.set_ylabel('FIALANG')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "fig = merged_df.boxplot(column='FIAPROXY')\n",
        "fig.set_title('')\n",
        "fig.set_ylabel('FIAPROXY')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "fig = merged_df.DMDBORN4.hist(bins=10)\n",
        "fig.set_xlabel('DMDBORN4')\n",
        "fig.set_ylabel('HIST')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "fig = merged_df.DMDCITZN.hist(bins=10)\n",
        "fig.set_xlabel('DMDCITZN')\n",
        "fig.set_ylabel('HIST')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "fig = merged_df.FIALANG.hist(bins=10)\n",
        "fig.set_xlabel('FIALANG')\n",
        "fig.set_ylabel('HIST')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "fig = merged_df.FIAPROXY.hist(bins=10)\n",
        "fig.set_xlabel('FIAPROXY')\n",
        "fig.set_ylabel('HIST')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MboECd-hSg7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Boxplots and histograms help visualize the distribution and detect outliers.\n",
        "- Outliers can be handled by removing or transforming the data if necessary.\n",
        "\n",
        "### **1.9. Prepare Target Variables**\n",
        "\n",
        "Load and preprocess the target variables for Cancer and Liver disease."
      ],
      "metadata": {
        "id": "Hs1q9FnnSjsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Questionnaire file\n",
        "target = pd.read_csv('/content/questionnaire.csv')\n",
        "\n",
        "# Replace '7's and '9's with NaN\n",
        "pattern = r'^9+$'\n",
        "target = target.applymap(replace_with_nan)\n",
        "pattern = r'^7+$'\n",
        "target = target.applymap(replace_with_nan)\n",
        "\n",
        "# Merge targets with the main dataset\n",
        "liver_values = target.merge(merged_df, on='SEQN', how='right')[['SEQN', 'MCQ160L']]\n",
        "cancer_values = target.merge(merged_df, on='SEQN', how='right')[['SEQN', 'MCQ220']]\n",
        "\n",
        "# Drop NaN values from target DataFrames\n",
        "cancer_values.dropna(subset=['MCQ220'], inplace=True)\n",
        "liver_values.dropna(subset=['MCQ160L'], inplace=True)\n",
        "\n",
        "# Merge back to get final datasets\n",
        "final_liver_data = liver_values.merge(merged_df, on='SEQN', how='left').drop(['SEQN', 'MCQ160L'], axis=1)\n",
        "final_cancer_data = cancer_values.merge(merged_df, on='SEQN', how='left').drop(['SEQN', 'MCQ220'], axis=1)\n",
        "\n",
        "# Drop 'SEQN' from target DataFrames\n",
        "cancer_values = cancer_values.drop(['SEQN'], axis=1)\n",
        "liver_values = liver_values.drop(['SEQN'], axis=1)\n",
        "\n",
        "final_cancer_data.head()\n",
        "final_liver_data.head()"
      ],
      "metadata": {
        "id": "cAX_7mR2SmcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- The target variables `MCQ220` (Cancer) and `MCQ160L` (Liver) indicate the presence (`1`) or absence (`2`) of the respective diseases.\n",
        "- Handle missing values by dropping rows with `NaN` in target variables.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Feature Engineering**\n",
        "\n",
        "Transform raw data into meaningful features to enhance model performance.\n",
        "\n",
        "### **2.1. Segregate Categorical and Numerical Features**\n"
      ],
      "metadata": {
        "id": "XNpmJTaPSows"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify categorical and numerical features for Cancer data\n",
        "cancer_categorical = ##TODO\n",
        "print(final_cancer_data[cancer_categorical].head())\n",
        "\n",
        "cancer_numerical = ##TODO\n",
        "print(final_cancer_data[cancer_numerical].head())\n",
        "\n",
        "# Identify categorical and numerical features for Liver data\n",
        "liver_categorical = ##TODO\n",
        "print(final_liver_data[liver_categorical].head())\n",
        "\n",
        "liver_numerical = ##TODO\n",
        "print(final_liver_data[liver_numerical].head())"
      ],
      "metadata": {
        "id": "trKFyq5vSuEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Categorical features are typically of type `object` or have a limited number of unique integer values.\n",
        "- Numerical features are of type `float64` or have many unique integer values.\n",
        "\n",
        "### **2.2. Impute Missing Values**\n",
        "\n",
        "#### **2.2.1. Numerical Features**\n",
        "\n",
        "Use median imputation for numerical features to handle missing values.\n"
      ],
      "metadata": {
        "id": "Uu-1lHeVSw3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values in Cancer numerical features\n",
        "for col in cancer_numerical:\n",
        "    col_median = ##TODO\n",
        "    final_cancer_data[col].##TODO\n",
        "\n",
        "# Impute missing values in Liver numerical features\n",
        "for col in liver_numerical:\n",
        "    col_median = ##TODO\n",
        "    final_liver_data[col].##TODO\n",
        "\n",
        "print(final_cancer_data[cancer_numerical].isnull().sum())\n",
        "print(final_liver_data[liver_numerical].isnull().sum())"
      ],
      "metadata": {
        "id": "En87KQGSS0J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Median imputation is robust to outliers and suitable for numerical data.\n",
        "- Replace `NaN` values with the median of each column.\n",
        "\n",
        "#### **2.2.2. Categorical Features**\n",
        "\n",
        "Use mode imputation for categorical features to handle missing values.\n"
      ],
      "metadata": {
        "id": "BHnvKjHFS16j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values in Cancer categorical features\n",
        "for col in cancer_categorical:\n",
        "    col_mode = ##TODO\n",
        "    final_cancer_data[col].##TODO\n",
        "\n",
        "# Impute missing values in Liver categorical features\n",
        "for col in liver_categorical:\n",
        "    col_mode = ##TODO\n",
        "    final_liver_data[col].##TODO\n",
        "\n",
        "print(final_cancer_data[cancer_categorical].isnull().sum())\n",
        "print(final_liver_data[categorical].isnull().sum())"
      ],
      "metadata": {
        "id": "fJUr0a-1S4jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Mode imputation fills missing categorical values with the most frequent category.\n",
        "- Ensure no missing values remain after imputation.\n",
        "\n",
        "### **2.3. Encode Categorical Variables**\n",
        "\n",
        "Convert categorical variables into numerical format using one-hot encoding.\n"
      ],
      "metadata": {
        "id": "QGqQ0uUZS7-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode categorical variables for Cancer data\n",
        "final_cancer_data = pd.##TODO\n",
        "\n",
        "# One-hot encode categorical variables for Liver data\n",
        "final_liver_data = pd.##TODO"
      ],
      "metadata": {
        "id": "bYsK28DXS9m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Use `pd.get_dummies()` to convert categorical variables into binary indicators.\n",
        "- This step is essential for feeding categorical data into the MLP.\n",
        "\n",
        "### **2.4. Feature Selection**\n",
        "\n",
        "Select the most relevant features based on correlation to reduce dimensionality and improve model performance.\n"
      ],
      "metadata": {
        "id": "s50EZzwfS_vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute correlation matrices\n",
        "cancer_corr_matrix = ##TODO\n",
        "liver_corr_matrix = ##TODO\n",
        "\n",
        "# Plot heatmaps for Cancer data\n",
        "abs_corr_matrix = np.abs(cancer_corr_matrix)\n",
        "cancer_corr_pairs = abs_corr_matrix.unstack().sort_values(ascending=False)\n",
        "top_pairs = cancer_corr_pairs[:1000]\n",
        "unique_pairs = top_pairs.drop_duplicates()\n",
        "top_columns = np.unique([col for col, _ in unique_pairs.keys()])\n",
        "top_corr_matrix = final_cancer_data[top_columns].corr()\n",
        "\n",
        "plt.figure(figsize=(50, 50))\n",
        "sns.heatmap(top_corr_matrix, annot=False, cmap='coolwarm')\n",
        "plt.show()\n",
        "\n",
        "# Plot heatmaps for Liver data\n",
        "abs_corr_matrix = np.abs(liver_corr_matrix)\n",
        "liver_corr_pairs = abs_corr_matrix.unstack().sort_values(ascending=False)\n",
        "top_pairs = liver_corr_pairs[:1000]\n",
        "unique_pairs = top_pairs.drop_duplicates()\n",
        "top_columns = np.unique([col for col, _ in unique_pairs.keys()])\n",
        "top_corr_matrix = final_liver_data[top_columns].corr()\n",
        "\n",
        "plt.figure(figsize=(50, 50))\n",
        "sns.heatmap(top_corr_matrix, annot=False, cmap='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "drWVk7e5TLLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- High correlation between features can lead to multicollinearity.\n",
        "- Selecting a subset of features with high correlation to the target reduces complexity.\n",
        "\n",
        "### **2.5. Select Top Features**\n",
        "\n",
        "Choose 150 features based on their correlation with the target variable.\n",
        "\n",
        "#### **2.5.1. Cancer Data**\n"
      ],
      "metadata": {
        "id": "lLBYj2UpTNdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Set correlation threshold\n",
        "threshold = 0.8\n",
        "\n",
        "# Get pairs with correlation higher than threshold\n",
        "high_corr_pairs = ##TODO\n",
        "\n",
        "# Initialize set for selected features\n",
        "selected_features = set()\n",
        "\n",
        "# Select features based on high correlation\n",
        "for pair, corr in high_corr_pairs:\n",
        "    selected_feature = ##TODO\n",
        "    selected_features.add(selected_feature)\n",
        "    if len(selected_features) == 150:\n",
        "        break\n",
        "\n",
        "# Warning if less than 150 features are selected\n",
        "if len(selected_features) < 150:\n",
        "    print(f\"Warning: Only {len(selected_features)} features were selected. Consider lowering the threshold.\")\n",
        "\n",
        "# Create DataFrame with selected features\n",
        "selected_cancer_df = final_cancer_data[list(selected_features)]"
      ],
      "metadata": {
        "id": "V3SPRWR7TT-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.5.2. Liver Data**"
      ],
      "metadata": {
        "id": "OiSpcVnvTWgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set correlation threshold\n",
        "threshold = 0.8\n",
        "\n",
        "# Get pairs with correlation higher than threshold\n",
        "high_corr_pairs = ##TODO\n",
        "\n",
        "# Initialize set for selected features\n",
        "selected_features = set()\n",
        "\n",
        "# Select features based on high correlation\n",
        "for pair, corr in high_corr_pairs:\n",
        "    selected_feature = ##TODO\n",
        "    selected_features.add(selected_feature)\n",
        "    if len(selected_features) == 150:\n",
        "        break\n",
        "\n",
        "# Warning if less than 150 features are selected\n",
        "if len(selected_features) < 150:\n",
        "    print(f\"Warning: Only {len(selected_features)} features were selected. Consider lowering the threshold.\")\n",
        "\n",
        "# Create DataFrame with selected features\n",
        "selected_liver_df = final_liver_data[list(selected_features)]"
      ],
      "metadata": {
        "id": "lpDynrzbTYv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Features with a correlation higher than the threshold are considered.\n",
        "- Randomly select one feature from highly correlated pairs to avoid redundancy.\n",
        "- Ensure that 150 features are selected; adjust the threshold if necessary.\n",
        "\n",
        "### **2.6. Scale the Features and Apply PCA**\n",
        "\n",
        "Normalize the data and reduce dimensionality using Principal Component Analysis (PCA).\n"
      ],
      "metadata": {
        "id": "SauUJAysTazg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale and apply PCA for Liver data\n",
        "cols = selected_liver_df.columns\n",
        "scaler = StandardScaler()\n",
        "selected_liver_df = scaler.fit_transform(selected_liver_df)\n",
        "selected_liver_df = pd.DataFrame(selected_liver_df, columns=[cols])\n",
        "\n",
        "pca = PCA(0.90)\n",
        "liverprincipalComponents = pca.fit_transform(selected_liver_df)\n",
        "liverprincipalDf = pd.DataFrame(data=liverprincipalComponents,\n",
        "                           columns=['Principal Component ' + str(i) for i in range(1, liverprincipalComponents.shape[1] + 1)])\n",
        "liverprincipalDf.head()\n",
        "\n",
        "# Scale and apply PCA for Cancer data\n",
        "cols = selected_cancer_df.columns\n",
        "scaler = StandardScaler()\n",
        "selected_cancer_df = scaler.fit_transform(selected_cancer_df)\n",
        "selected_cancer_df = pd.DataFrame(selected_cancer_df, columns=[cols])\n",
        "\n",
        "pca = PCA(0.90)\n",
        "cancerprincipalComponents = pca.fit_transform(selected_cancer_df)\n",
        "cancerprincipalDf = pd.DataFrame(data=cancerprincipalComponents,\n",
        "                           columns=['Principal Component ' + str(i) for i in range(1, cancerprincipalComponents.shape[1] + 1)])\n",
        "cancerprincipalDf.head()"
      ],
      "metadata": {
        "id": "t5kxBvYHTd8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- `StandardScaler` standardizes features by removing the mean and scaling to unit variance.\n",
        "- PCA reduces the number of features while retaining 90% of the variance, simplifying the model.\n",
        "\n",
        "### **2.7. Encode Target Labels**\n",
        "\n",
        "Convert target labels from `1` and `2` to `0` and `1`.\n"
      ],
      "metadata": {
        "id": "NCyql4B9TgJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Cancer labels: 1 -> 1 (present), 2 -> 0 (absent)\n",
        "cancer_values[##TODO] = 0\n",
        "cancer_values.head()\n",
        "\n",
        "# Convert Liver labels: 1 -> 1 (present), 2 -> 0 (absent)\n",
        "liver_values[##TODO] = 0\n",
        "liver_values.head()"
      ],
      "metadata": {
        "id": "acUf-2mWTiUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Binary classification requires labels to be `0` and `1`.\n",
        "- Replace `2` with `0` to indicate absence of disease.\n",
        "\n",
        "### **2.8. Split the Data into Training and Testing Sets**\n",
        "\n",
        "Divide the dataset into training and testing subsets.\n"
      ],
      "metadata": {
        "id": "oV9NT0AYTk12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Cancer data\n",
        "cancer_x_train, cancer_x_test, cancer_y_train, cancer_y_test = train_test_split(\n",
        "    cancerprincipalDf, cancer_values, test_size=0.2, random_state=0)\n",
        "\n",
        "# Split Liver data\n",
        "liver_x_train, liver_x_test, liver_y_train, liver_y_test = train_test_split(\n",
        "    liverprincipalDf, liver_values, test_size=0.2, random_state=0)\n",
        "\n",
        "print(cancer_x_train.shape, cancer_x_test.shape, liver_x_train.shape, liver_x_test.shape)"
      ],
      "metadata": {
        "id": "Ej3jGBRDTm19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Use an 80/20 split for training and testing.\n",
        "- Ensures that the model is trained on a majority of the data and tested on unseen data.\n",
        "\n",
        "### **2.9. Handle Class Imbalance**\n",
        "\n",
        "Use under-sampling to balance the classes in the training data.\n"
      ],
      "metadata": {
        "id": "np6AVWPeTpnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Under-sampling for Liver dataset\n",
        "rus = RandomUnderSampler(random_state=0)\n",
        "liver_x_resampled, liver_y_resampled = rus.fit_resample(liver_x_train, liver_y_train)\n",
        "\n",
        "# Under-sampling for Cancer dataset\n",
        "rus = RandomUnderSampler(random_state=0)\n",
        "cancer_x_resampled, cancer_y_resampled = rus.fit_resample(cancer_x_train, cancer_y_train)"
      ],
      "metadata": {
        "id": "sf_VS9tATrZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Imbalanced datasets can bias the model towards the majority class.\n",
        "- Under-sampling reduces the size of the majority class to balance the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Implementing MLP from Scratch**\n",
        "\n",
        "In this section, you will implement a simple MLP with one hidden layer using NumPy. The network will consist of an input layer, a hidden layer with 10 neurons, and an output layer. The sigmoid activation function will be used, and Binary Cross-Entropy will be the loss function.\n",
        "\n",
        "### **3.1. Define Data Preprocessor Class**\n",
        "\n",
        "This class handles splitting the resampled data into training and validation sets."
      ],
      "metadata": {
        "id": "duCUAdFFTuod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreprocessor:\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Initialize with resampled training data and test data\n",
        "        '''\n",
        "        self.X = ##TODO\n",
        "        self.y = ##TODO\n",
        "        self.X_test = ##TODO\n",
        "        self.y_test = ##TODO\n",
        "\n",
        "    def preprocess(self):\n",
        "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
        "            self.X, self.y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "crDa9sHfTw2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Initializes with resampled training data (`X` and `y`) and test data.\n",
        "- Splits the training data further into training and validation sets for monitoring overfitting.\n",
        "\n",
        "### **3.2. Define MLP Class**\n",
        "\n",
        "This class defines the structure and operations of the MLP.\n"
      ],
      "metadata": {
        "id": "o1-ZV0ZQTzuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        '''\n",
        "        Initialize weights and biases\n",
        "        '''\n",
        "        self.weights1 = ##TODO\n",
        "        self.bias1 = ##TODO\n",
        "        self.weights2 = ##TODO\n",
        "        self.bias2 = ##TODO\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        '''\n",
        "        Implement the sigmoid activation function\n",
        "        '''\n",
        "        ##TODO\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        '''\n",
        "        Implement the derivative of the sigmoid function\n",
        "        '''\n",
        "        ##TODO\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Perform forward propagation\n",
        "        '''\n",
        "        self.z1 = ##TODO\n",
        "        self.a1 = ##TODO\n",
        "        self.z2 = ##TODO\n",
        "        self.a2 = ##TODO\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate):\n",
        "        '''\n",
        "        Perform backward propagation and update weights and biases\n",
        "        '''\n",
        "        m = X.shape[0]\n",
        "\n",
        "        dz2 = ##TODO\n",
        "        dw2 = ##TODO\n",
        "        db2 = ##TODO\n",
        "\n",
        "        dz1 = ##TODO\n",
        "        dw1 = ##TODO\n",
        "        db1 = ##TODO\n",
        "\n",
        "        self.weights2 -= ##TODO\n",
        "        self.bias2 -= ##TODO\n",
        "        self.weights1 -= ##TODO\n",
        "        self.bias1 -= ##TODO"
      ],
      "metadata": {
        "id": "7g3FDzwnT2ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- **Initialization (`__init__`):**\n",
        "  - Initialize weights with small random values.\n",
        "  - Initialize biases with zeros.\n",
        "  \n",
        "- **Activation Functions:**\n",
        "  - `sigmoid`: Activation function for neurons.\n",
        "  - `sigmoid_derivative`: Derivative used during backpropagation.\n",
        "  \n",
        "- **Forward Propagation (`forward`):**\n",
        "  - Compute activations for hidden and output layers.\n",
        "  \n",
        "- **Backward Propagation (`backward`):**\n",
        "  - Compute gradients and update weights and biases using Stochastic Gradient Descent (SGD).\n",
        "\n",
        "### **3.3. Define Trainer Class**\n",
        "\n",
        "This class handles the training process, including batching, loss calculation, and accuracy monitoring."
      ],
      "metadata": {
        "id": "h9F3wW-dT84R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, X_train, y_train, X_val, y_val, epochs=100, learning_rate=0.1, batch_size=32):\n",
        "        self.model = model\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_losses = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def train(self):\n",
        "        m = self.X_train.shape[0]\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Shuffle the training data\n",
        "            indices = ##TODO\n",
        "            X_shuffled = self.X_train[indices]\n",
        "            y_shuffled = self.y_train[indices]\n",
        "\n",
        "            for i in range(0, m, self.batch_size):\n",
        "                # Get the current batch\n",
        "                X_batch = ##TODO\n",
        "                y_batch = ##TODO\n",
        "\n",
        "                # Forward pass\n",
        "                output = ##TODO\n",
        "\n",
        "                # Backward pass\n",
        "                self.model.backward(##TODO)\n",
        "\n",
        "            # Calculate training loss and accuracy\n",
        "            train_output = ##TODO\n",
        "            train_loss = ##TODO\n",
        "            train_accuracy = ##TODO\n",
        "\n",
        "            # Calculate validation loss and accuracy\n",
        "            val_output = ##TODO\n",
        "            val_loss = ##TODO\n",
        "            val_accuracy = ##TODO\n",
        "\n",
        "            # Store the metrics\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.train_accuracies.append(train_accuracy)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accuracies.append(val_accuracy)\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{self.epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "    def plot_results(self):\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.train_losses, label='Train')\n",
        "        plt.plot(self.val_losses, label='Validation')\n",
        "        plt.title('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.train_accuracies, label='Train')\n",
        "        plt.plot(self.val_accuracies, label='Validation')\n",
        "        plt.title('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "-0zWoojeUA8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- **Training Process (`train`):**\n",
        "  - Shuffles the data at each epoch to ensure randomness.\n",
        "  - Processes data in batches for efficient computation.\n",
        "  - Performs forward and backward passes to update the model.\n",
        "  - Calculates and stores loss and accuracy for both training and validation sets.\n",
        "  \n",
        "- **Plotting Results (`plot_results`):**\n",
        "  - Visualizes the loss and accuracy over epochs to monitor training progress and detect overfitting.\n",
        "\n",
        "### **3.4. Define Tester Class**\n",
        "\n",
        "This class evaluates the trained model on the test dataset.\n"
      ],
      "metadata": {
        "id": "7idPxKgBUDHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tester:\n",
        "    def __init__(self, model, X_test, y_test):\n",
        "        self.model = model\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "    def test(self):\n",
        "        '''\n",
        "        Perform testing by forwarding test data through the model and computing loss and accuracy\n",
        "        '''\n",
        "        test_output = ##TODO\n",
        "        test_loss = ##TODO\n",
        "        test_accuracy = ##TODO\n",
        "\n",
        "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "        return test_output"
      ],
      "metadata": {
        "id": "0MS7-7QGUFcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- **Testing Process (`test`):**\n",
        "  - Performs a forward pass with the test data.\n",
        "  - Computes the Binary Cross-Entropy loss and accuracy.\n",
        "  - Prints and returns the test results.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Training and Evaluating the Model**\n",
        "\n",
        "Finally, you will tie everything together to train and evaluate your MLP model.\n",
        "\n",
        "### **4.1. Main Execution**"
      ],
      "metadata": {
        "id": "FPU0U_JbUH0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Preprocess data\n",
        "    preprocessor = DataPreprocessor()\n",
        "    preprocessor.preprocess()\n",
        "\n",
        "    # Initialize and train model\n",
        "    input_size = preprocessor.X_train.shape[1]\n",
        "    hidden_size = 10\n",
        "    output_size = 1\n",
        "\n",
        "    model = MLP(input_size, hidden_size, output_size)\n",
        "    trainer = Trainer(model, preprocessor.X_train, preprocessor.y_train,\n",
        "                      preprocessor.X_val, preprocessor.y_val,\n",
        "                      epochs=100, learning_rate=0.1, batch_size=32)\n",
        "    trainer.train()\n",
        "\n",
        "    # Plot training results\n",
        "    trainer.plot_results()\n",
        "\n",
        "    # Test the model\n",
        "    tester = Tester(model, preprocessor.X_test, preprocessor.y_test)\n",
        "    test_output = tester.test()\n",
        "\n",
        "    # Print some example predictions\n",
        "    print(\"\\nExample predictions:\")\n",
        "    for i in range(5):\n",
        "        true_label = np.argmax(preprocessor.y_test[i])\n",
        "        predicted_label = np.argmax(test_output[i])\n",
        "        print(f\"True: {true_label}, Predicted: {predicted_label}\")"
      ],
      "metadata": {
        "id": "0yxfma0eUJ_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- **Data Preprocessing:**\n",
        "  - Initializes the `DataPreprocessor` and splits the data.\n",
        "  \n",
        "- **Model Initialization and Training:**\n",
        "  - Initializes the `MLP` with appropriate sizes.\n",
        "  - Creates a `Trainer` instance and trains the model.\n",
        "  \n",
        "- **Plotting Results:**\n",
        "  - Visualizes the training and validation loss and accuracy.\n",
        "  \n",
        "- **Testing:**\n",
        "  - Evaluates the model on the test dataset.\n",
        "  - Prints example predictions to compare true and predicted labels.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Instructions for Completing the Assignment**\n",
        "\n",
        "Throughout the notebook, you will encounter `##TODO` sections. Your tasks are to:\n",
        "\n",
        "1. **Load and Merge Datasets:**\n",
        "   - Use `pd.read_csv()` to load each CSV file into a DataFrame.\n",
        "   - Merge the DataFrames on the `SEQN` column using `pd.merge()`.\n",
        "\n",
        "2. **Identify and Drop Columns:**\n",
        "   - Use `.nunique()` to find columns with a single unique value.\n",
        "   - Drop these columns using `.drop()`.\n",
        "\n",
        "3. **Handle Missing Values:**\n",
        "   - Implement the `replace_with_nan` function to replace specific patterns with `NaN`.\n",
        "   - Use `.applymap()` to apply this function across the DataFrame.\n",
        "   - Drop columns with more than 30% missing values.\n",
        "\n",
        "4. **Segregate Numerical and Categorical Variables:**\n",
        "   - Identify numerical columns (typically `float64` or `int64` with many unique values).\n",
        "   - Identify categorical columns (typically `object` or `int64` with few unique values).\n",
        "\n",
        "5. **Impute Missing Values:**\n",
        "   - For numerical columns, compute the median and replace `NaN` values.\n",
        "   - For categorical columns, compute the mode and replace `NaN` values.\n",
        "\n",
        "6. **Encode Categorical Variables:**\n",
        "   - Use `pd.get_dummies()` to perform one-hot encoding on categorical columns.\n",
        "\n",
        "7. **Feature Selection:**\n",
        "   - Compute the correlation matrix using `.corr()`.\n",
        "   - Select top features based on correlation threshold.\n",
        "   - Ensure that 150 features are selected, adjusting the threshold if necessary.\n",
        "\n",
        "8. **Scale and Apply PCA:**\n",
        "   - Use `StandardScaler` to normalize the data.\n",
        "   - Apply PCA to reduce dimensionality while retaining 90% variance.\n",
        "\n",
        "9. **Encode Target Labels:**\n",
        "   - Convert disease labels from `1` and `2` to `0` and `1`.\n",
        "\n",
        "10. **Handle Class Imbalance:**\n",
        "    - Use `RandomUnderSampler` to balance the classes in the training data.\n",
        "\n",
        "11. **Implement MLP:**\n",
        "    - Initialize weights and biases with appropriate dimensions and values.\n",
        "    - Implement the sigmoid activation function and its derivative.\n",
        "    - Perform forward and backward propagation.\n",
        "    - Update weights and biases using gradients.\n",
        "\n",
        "12. **Train and Evaluate the Model:**\n",
        "    - Train the MLP using the `Trainer` class.\n",
        "    - Plot the loss and accuracy curves.\n",
        "    - Test the model and evaluate its performance.\n",
        "\n",
        "---\n",
        "\n",
        "## **Submission Guidelines**\n",
        "\n",
        "- **Notebook Completion:** Ensure all `##TODO` sections are properly implemented.\n",
        "- **Code Quality:** Write clean, readable, and well-documented code.\n",
        "- **Visualizations:** Include relevant plots for EDA and training progress.\n",
        "- **Report:** Summarize your findings, challenges faced, and insights gained from the model's performance.\n",
        "\n",
        "---\n",
        "\n",
        "Good luck with your assignment! If you have any questions or need further clarifications, feel free to reach out."
      ],
      "metadata": {
        "id": "RZ2ty7_3UQbv"
      }
    }
  ]
}