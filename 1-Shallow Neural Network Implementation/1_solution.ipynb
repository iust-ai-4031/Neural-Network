{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (9, 534), y_train.shape: (534,)\n",
      "X_validation.shape: (9, 134), y_validation.shape: (134,)\n",
      "test_data_numpy.shape: (100, 9)\n",
      "Epoch 1/100 - Accuracy: 40.45%\n",
      "Epoch 10/100 - Accuracy: 55.43%\n",
      "Epoch 20/100 - Accuracy: 67.04%\n",
      "Epoch 30/100 - Accuracy: 72.10%\n",
      "Epoch 40/100 - Accuracy: 73.03%\n",
      "Epoch 50/100 - Accuracy: 73.60%\n",
      "Epoch 60/100 - Accuracy: 74.53%\n",
      "Epoch 70/100 - Accuracy: 74.91%\n",
      "Epoch 80/100 - Accuracy: 75.28%\n",
      "Epoch 90/100 - Accuracy: 75.28%\n",
      "Epoch 100/100 - Accuracy: 75.47%\n",
      "Model accuracy on validation set: 70.90%\n",
      "Test Predictions: [0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =========================\n",
    "# Data Loading and Preprocessing\n",
    "# =========================\n",
    "\n",
    "# Load training and test data\n",
    "train_data = pd.read_csv('data/diabetes_train.csv')\n",
    "test_data = pd.read_csv('data/diabetes_test.csv')\n",
    "\n",
    "# Preprocessing - Separating target and features\n",
    "train_data_outcome = train_data['Outcome'].values  # Shape: (668,)\n",
    "train_data = train_data.drop(columns=['Outcome'])    # Shape: (668, 8)\n",
    "\n",
    "# Normalizing features using training data statistics\n",
    "for column in train_data.columns:\n",
    "    mean = train_data[column].mean()\n",
    "    std = train_data[column].std()\n",
    "    train_data[column] = (train_data[column] - mean) / std\n",
    "    test_data[column] = (test_data[column] - mean) / std\n",
    "\n",
    "# Adding bias column (intercept term) with value 1\n",
    "train_data.insert(0, 'Bias', 1)  # Shape: (668, 9)\n",
    "test_data.insert(0, 'Bias', 1)    # Shape: (100, 9)\n",
    "\n",
    "# Convert DataFrames to numpy arrays\n",
    "train_data = train_data.values       # Shape: (668, 9)\n",
    "test_data_numpy = test_data.values   # Shape: (100, 9)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# Note: train_test_split expects samples as rows, so no transposing here\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    train_data, train_data_outcome, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Transpose the feature matrices to shape (features, samples) for the model\n",
    "X_train = X_train.T          # Shape: (9, 534)\n",
    "X_validation = X_validation.T  # Shape: (9, 134)\n",
    "\n",
    "# Target variables remain as 1D arrays\n",
    "y_train = y_train             # Shape: (534,)\n",
    "y_validation = y_validation   # Shape: (134,)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f'X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}')\n",
    "print(f'X_validation.shape: {X_validation.shape}, y_validation.shape: {y_validation.shape}')\n",
    "print(f'test_data_numpy.shape: {test_data_numpy.shape}')\n",
    "\n",
    "# =========================\n",
    "# Model Definition\n",
    "# =========================\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, input_size, hidden_size=1000, output_size=1):\n",
    "        # Initialize weights with mean 0 and std 0.01\n",
    "        self.w1 = np.random.randn(hidden_size, input_size) * 0.01  # Shape: (1000, 9)\n",
    "        self.w2 = np.random.randn(output_size, hidden_size) * 0.01  # Shape: (1, 1000)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network.\n",
    "        Args:\n",
    "            inputs (numpy.ndarray): Input data of shape (features, samples)\n",
    "        Returns:\n",
    "            A_1 (numpy.ndarray): Activations from hidden layer\n",
    "            A_2 (numpy.ndarray): Activations from output layer\n",
    "        \"\"\"\n",
    "        Z_1 = self.w1 @ inputs          # Shape: (1000, samples)\n",
    "        A_1 = np.maximum(0, Z_1)        # ReLU activation\n",
    "        Z_2 = self.w2 @ A_1             # Shape: (1, samples)\n",
    "        A_2 = 1 / (1 + np.exp(-Z_2))    # Sigmoid activation\n",
    "        return A_1, A_2\n",
    "\n",
    "    def update_weights_for_one_epoch(self, inputs, outputs, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one epoch of weight updates using gradient descent.\n",
    "        Args:\n",
    "            inputs (numpy.ndarray): Input data of shape (features, samples)\n",
    "            outputs (numpy.ndarray): True labels of shape (samples,)\n",
    "            learning_rate (float): Learning rate (alpha)\n",
    "        \"\"\"\n",
    "        A_1, A_2 = self.predict(inputs)  # Forward pass\n",
    "\n",
    "        n = inputs.shape[1]  # Number of samples\n",
    "\n",
    "        # Compute the error term for output layer\n",
    "        error_output = (outputs - A_2.flatten())  # Shape: (samples,)\n",
    "        shared_coefficient = (2 / n) * error_output * A_2.flatten() * (1 - A_2.flatten())  # Shape: (samples,)\n",
    "\n",
    "        # Reshape shared_coefficient for matrix operations\n",
    "        shared_coefficient = shared_coefficient.reshape(1, -1)  # Shape: (1, samples)\n",
    "\n",
    "        # Update weights for w2\n",
    "        delta_w2 = shared_coefficient @ A_1.T  # Shape: (1, 1000)\n",
    "        self.w2 += learning_rate * delta_w2\n",
    "\n",
    "        # Compute the gradient for w1\n",
    "        delta_A1 = self.w2.T @ shared_coefficient  # Shape: (1000, samples)\n",
    "        relu_gradient = np.where(A_1 > 0, 1, 0)    # Shape: (1000, samples)\n",
    "        delta_w1 = (delta_A1 * relu_gradient) @ inputs.T  # Shape: (1000, 9)\n",
    "        self.w1 += learning_rate * delta_w1\n",
    "\n",
    "    def fit(self, inputs, outputs, learning_rate, epochs=64):\n",
    "        \"\"\"\n",
    "        Trains the model using the provided data.\n",
    "        Args:\n",
    "            inputs (numpy.ndarray): Input data of shape (features, samples)\n",
    "            outputs (numpy.ndarray): True labels of shape (samples,)\n",
    "            learning_rate (float): Learning rate (alpha)\n",
    "            epochs (int): Number of training epochs\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.update_weights_for_one_epoch(inputs, outputs, learning_rate)\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                _, A_2 = self.predict(inputs)\n",
    "                predictions = (A_2.flatten() > 0.5).astype(int)\n",
    "                accuracy = np.mean(predictions == outputs) * 100\n",
    "                print(f'Epoch {epoch + 1}/{epochs} - Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# =========================\n",
    "# Model Training and Evaluation\n",
    "# =========================\n",
    "\n",
    "def evaluation(model, inputs, outputs):\n",
    "    \"\"\"\n",
    "    Evaluates the model's accuracy.\n",
    "    Args:\n",
    "        model (Model): Trained model\n",
    "        inputs (numpy.ndarray): Input data of shape (features, samples)\n",
    "        outputs (numpy.ndarray): True labels of shape (samples,)\n",
    "    Returns:\n",
    "        float: Accuracy percentage\n",
    "    \"\"\"\n",
    "    _, A_2 = model.predict(inputs)\n",
    "    prediction = (A_2.flatten() > 0.5).astype(int)\n",
    "    return np.mean(prediction == outputs) * 100\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[0]  # Number of features (including bias)\n",
    "model = Model(input_size=input_size, hidden_size=1000, output_size=1)\n",
    "\n",
    "# Train the model\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "model.fit(X_train, y_train, learning_rate=learning_rate, epochs=epochs)\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "validation_accuracy = evaluation(model, X_validation, y_validation)\n",
    "print(f\"Model accuracy on validation set: {validation_accuracy:.2f}%\")\n",
    "\n",
    "# =========================\n",
    "# Prediction on Test Data\n",
    "# =========================\n",
    "\n",
    "# Make predictions on test data\n",
    "_, test_output = model.predict(test_data_numpy.T)  # Transpose to shape (9, 100)\n",
    "prediction = (test_output.flatten() > 0.5).astype(int)\n",
    "\n",
    "# Display test predictions\n",
    "print(\"Test Predictions:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
