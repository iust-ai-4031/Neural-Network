{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAe8Vr1cIT4u"
      },
      "source": [
        "# Shallow Neural Network Implementation\n",
        "\n",
        "## Importing Required Libraries\n",
        "\n",
        "First, import the necessary libraries. Note that in this assignment, you are only allowed to use the libraries provided in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGq-ZgsQIWEn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf_FyNaqIaje"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "In this exercise, we will use the simple yet famous **Pima Indians Diabetes** dataset. This dataset includes information from **768 Native American women** from the Pima tribe, collected to examine the risk factors for developing type 2 diabetes. The data includes age, weight, height, family history of diabetes, blood pressure, blood glucose levels, and other factors.\n",
        "\n",
        "<center>\n",
        "<div style=\"line-height:200%; font-size:medium\">\n",
        "    \n",
        "| Column | Description |\n",
        "|:------:|:-----------:|\n",
        "|Pregnancies|Number of pregnancies|\n",
        "|Glucose|Blood glucose level (mg/dL)|\n",
        "|BloodPressure|Systolic blood pressure (mmHg)|\n",
        "|SkinThickness|Skin thickness (mm)|\n",
        "|Insulin|Blood insulin level (μU/mL)|\n",
        "|BMI|Body mass index (kg/m²)|\n",
        "|DiabetesPedigreeFunction|Function representing family history of diabetes|\n",
        "|Age|Age of the woman (years)|\n",
        "|Outcome|Non-diabetic (0) or diabetic (1)|\n",
        "\n",
        "</div>\n",
        "</center>\n",
        "\n",
        "### Reading the Dataset\n",
        "\n",
        "First, you need to read the dataset file. You can read the training data from the file `diabetes_train.csv` located in the `data` folder and use the samples in it to train the model. The model's performance will be evaluated on `diabetes_test.csv`, which has the same structure as the training data except that the `Outcome` column is removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fYf0j_6Iljd"
      },
      "outputs": [],
      "source": [
        "train_data = None # TODO\n",
        "train_data.head()\n",
        "test_data = None # TODO\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM5Fb_wTIwoy"
      },
      "source": [
        "## Preprocessing and Feature Engineering\n",
        "\n",
        "First, store the target variable column (`Outcome`) in a separate DataFrame and then remove this column from the `train_data` DataFrame to create the equivalent matrices $X$ and $y$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgwFWO9DI4Kq"
      },
      "outputs": [],
      "source": [
        "train_data_outcome = None # TODO\n",
        "train_data = None # TODO: drop Outcome\n",
        "\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1TOYDG5I5cu"
      },
      "source": [
        "One of the crucial preprocessing steps is feature scaling to a normal distribution, commonly referred to as normalization. Normalization helps reduce significant weight fluctuations and accelerates model convergence. In this assignment, you should normalize each feature so that their mean is `0` and their variance is `1`. This can be done using the following formula:\n",
        "\n",
        "For a data series `X = [x_1, x_2, ..., x_n]`, subtract the mean from each data sample (`x_i`) and divide by the standard deviation (sigma) to obtain the normalized data series.\n",
        "\n",
        "$$ Z = \\frac{x_i - \\bar{x}}{\\sigma} $$\n",
        "\n",
        "**Note:** Since we only have access to the training data when building the model, use the mean and standard deviation from the training samples to normalize the test samples as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJddsz0JI88N"
      },
      "outputs": [],
      "source": [
        "for column in train_data.columns:\n",
        "  mean = None # TODO (Mean of train_data[column])\n",
        "  std = None # TODO (Standard Deviation of train_data[column])\n",
        "  train_data[column] = (train_data[column] - mean) / std\n",
        "  test_data[column] = (test_data[column] - mean) / std\n",
        "\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e81ZduqxI_W1"
      },
      "source": [
        "Next, add a bias term to the DataFrame. To do this, add a column with a value of `1` at the beginning of the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXFJbXe0JDzL"
      },
      "outputs": [],
      "source": [
        "train_bias = None # TODO\n",
        "train_data = None # TODO: add Bias to train_data\n",
        "\n",
        "test_bias = None # TODO\n",
        "test_data = None # TODO: add Bias to test_data\n",
        "\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atOwCWIKJEhG"
      },
      "source": [
        "Before designing and training the model, convert the datasets from DataFrames to NumPy arrays. Therefore, in this step, convert the DataFrames `train_data` and `train_data_outcome` to NumPy arrays. Additionally, use the `train_test_split` function to split this dataset into training and validation sets with a ratio of `0.2`.\n",
        "\n",
        "**Note:** According to previous lectures, each **row** of the input matrix represents a **feature**, and each **column** represents a **sample**. Therefore, you need to transpose the feature matrix. This step should also be applied to the target variable (`train_data_outcome`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Moa4vZOHJHx1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(None, None, test_size=0.2)\n",
        "\n",
        "X_train = None # TODO\n",
        "X_validation = None # TODO\n",
        "y_train = None # TODO\n",
        "y_validation = None # TODO\n",
        "test_data_numpy = None # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdHMNuKjJLtk"
      },
      "source": [
        "To ensure the correctness of input and output settings, running the next cell should produce the following output:\n",
        "\n",
        "```\n",
        "X_train.shape:(9, 534), y_train.shape:(534,)\n",
        "X_validation.shape:(9, 134), y_validation.shape:(134,)\n",
        "test_data_numpy.shape:(9, 100)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3W2d5xQJMT1"
      },
      "outputs": [],
      "source": [
        "print(f'X_train.shape:{X_train.shape}, y_train.shape:{y_train.shape}')\n",
        "print(f'X_validation.shape:{X_validation.shape}, y_validation.shape:{y_validation.shape}')\n",
        "print(f'test_data_numpy.shape:{test_data_numpy.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5I0--wwJ83T"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "Now that the data is processed and ready, it's time for the main part—building the model. You are required to implement a simple shallow neural network using gradient descent from scratch. We will explain each component of this model step by step to guide you through its implementation.\n",
        "\n",
        "This model is a shallow neural network with one hidden layer containing `1000` neurons. The activation function for this layer is the Rectified Linear Unit (ReLU), which you are familiar with from the activation function lectures. The activation function for the output layer is the sigmoid function. Note that the required formulas for each part are provided below. We have also implemented the two activation functions for you.\n",
        "\n",
        "```python\n",
        "sigmoid_Z = 1 / (1 + np.exp(-Z))\n",
        "```\n",
        "\n",
        "```python\n",
        "ReLU_Z = np.maximum(0, Z)\n",
        "```\n",
        "\n",
        "**Note:** Only use the NumPy library for mathematical operations and computations, and define your lists as NumPy arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvPJycbcKTGh"
      },
      "source": [
        "### Reminder: Sigmoid Function\n",
        "\n",
        "| Sigmoid Function | Derivative of Sigmoid Function |\n",
        "| :---: | :--: |\n",
        "| $f(z) = \\frac{1}{1 + e^{-z}}$ | $f'(z) = f(z)(1-f(z))$ |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPw_rHtcKUmZ"
      },
      "source": [
        "### Reminder: ReLU Activation Function\n",
        "\n",
        "| ReLU Function  | Derivative of ReLU Function  |\n",
        "| :---: | :--: |\n",
        "|$$f(z) = \\begin{cases} 0 & \\text{if } z < 0 \\\\ z & \\text{if } z \\geq 0\\end{cases}$$|$$f'(z) = \\begin{cases} 0 & \\text{if } z < 0 \\\\ 1 & \\text{if } z \\geq 0\\end{cases}$$|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yIwnRmKZMx"
      },
      "source": [
        "### `Model` Class Construction\n",
        "\n",
        "Create a class named `Model` that contains the following three methods. We will explain each method in detail below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CI6f_Ax3KgMm"
      },
      "outputs": [],
      "source": [
        "def __init__(self)\n",
        "def predict(self, inputs)\n",
        "def update_weights_for_one_epoch(self, inputs, outputs, learning_rate)\n",
        "def fit(self, inputs, outputs, learning_rate, epochs=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLqB5ZibKivK"
      },
      "source": [
        "#### `__init__` Method\n",
        "\n",
        "In the `__init__(self)` method, initialize the weights of the hidden and output layers (`w1` and `w2`) randomly with a mean of `0` and a standard deviation of `0.01`. You can use the `np.random.randn` function for this purpose. Note that `np.random.randn` generates random numbers with a mean of `0` and a standard deviation of `1`, so you need to adjust these values accordingly to meet the problem requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNjFJpqfKl-o"
      },
      "source": [
        "#### `predict` Method\n",
        "\n",
        "The `predict(self, inputs)` method takes the inputs and sequentially returns the outputs of both layers (`A_1` and `A_2`). Implement this process according to the following formulas:\n",
        "\n",
        "$$Z^{[1]}=W^{[1]}.X$$\n",
        "$$A^{[1]}=ReLU(Z^{[1]})$$\n",
        "$$Z^{[2]}=W^{[2]}A^{[1]}$$\n",
        "$$A^{[2]}=\\sigma(Z^{[2]})=\\frac{1}{1+e^{-Z^{[2]}}}=Y_{pred}$$\n",
        "\n",
        "**Hint:** To perform matrix multiplication between two matrices, use the `arr1.dot(arr2)` function. For example, the formula $Z^{[1]}=W^{[1]}X$ corresponds to `W_1.dot(X)` in Python. Alternatively, you can use the `@` operator as `W_1 @ X`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_-V2fChKqsT"
      },
      "source": [
        "#### `update_weights_for_one_epoch` Method\n",
        "\n",
        "In the `update_weights_for_one_epoch(self, inputs, outputs, learning_rate)` method, update the network's weights for one epoch. Note that `learning_rate` is the learning rate or alpha. The required formulas for this section are provided below. In the next chapter, we will explain in detail how to compute them.\n",
        "\n",
        "**Weight Update for `w2`:**\n",
        "\n",
        "$$W^{[2]} = W^{[2]} + \\Delta W^{[2]}$$\n",
        "$$\\Delta W^{[2]} = - \\alpha \\frac{\\partial cost}{\\partial W^{[2]}}$$\n",
        "$$\\frac{\\partial cost}{\\partial W^{[2]}} = \\left(\\frac{-2}{n}(Y_{true}-A^{[2]}) \\odot A^{[2]} \\odot (1-A^{[2]})\\right) \\bullet A^{[1]T}$$\n",
        "$$W^{[2]} = W^{[2]} + \\left(\\frac{2 \\alpha}{n}(Y_{true}-A^{[2]}) \\odot A^{[2]} \\odot (1-A^{[2]})\\right) \\bullet A^{[1]T}$$\n",
        "\n",
        "**Weight Update for `w1`:**\n",
        "\n",
        "$$W^{[1]} = W^{[1]} + \\Delta W^{[1]}$$\n",
        "$$\\Delta W^{[1]} = - \\alpha \\frac{\\partial cost}{\\partial W^{[1]}}$$\n",
        "\n",
        "$$\\frac{\\partial cost}{\\partial W^{[1]}} = \\left(\\left(\\frac{-2}{n}(Y_{true}-A^{[2]}) \\odot A^{[2]} \\odot (1-A^{[2]})\\right)^T \\bullet W^{[2]}\\right)^T \\odot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\bullet X^T$$\n",
        "\n",
        "$$W^{[1]} = W^{[1]} + \\left(\\left(\\frac{2 \\alpha}{n}(Y_{true}-A^{[2]}) \\odot A^{[2]} \\odot (1-A^{[2]})\\right)^T \\bullet W^{[2]}\\right)^T \\odot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\bullet X^T$$\n",
        "\n",
        "**Note:** The symbol $\\odot$ represents element-wise multiplication, and the symbol $\\bullet$ represents matrix multiplication.\n",
        "\n",
        "To obtain the value of $\\frac{\\partial A^{[1]}}{\\partial Z^{[1]}}$, which is the derivative of the ReLU function, use the following code snippet. This will produce a matrix of the same size as $Z^{[1]}$, composed of `0` and `1`, where cells corresponding to $Z^{[1]} > 0$ will have a value of `1`, and `0` otherwise. Note that although you pass `A_1` as input to this function, it does not affect the output.\n",
        "\n",
        "```python\n",
        "relu_gradient = np.where(A_1 > 0, 1, 0)\n",
        "```\n",
        "\n",
        "**Important:** Part of $\\Delta W^{[1]}$ is already computed in $\\Delta W^{[2]}$. By storing it, you can avoid redundant calculations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMV3g1W6K0Qy"
      },
      "source": [
        "#### `fit` Method\n",
        "\n",
        "The `fit(self, inputs, outputs, learning_rate, epochs=64)` method updates the network's weights for the specified number of epochs (`epochs`). You do not need to make any changes to this method; simply use it in the subsequent steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ7Z6vUxK3rW"
      },
      "source": [
        "### Model Class Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukBhc2a7K836"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.w1 = None # TODO: random matrix with size (1000, number_of_input_features)\n",
        "        self.w2 = None # TODO: random matrix with size (1, 1000)\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        x = inputs\n",
        "\n",
        "        Z_1 = None # TODO\n",
        "        A_1 = np.maximum(0, None) # TODO\n",
        "\n",
        "        Z_2 = None # TODO\n",
        "        A_2 = 1 / (1 + np.exp(-None)) # TODO\n",
        "\n",
        "        return A_1, A_2\n",
        "\n",
        "    def update_weights_for_one_epoch(self, inputs, outputs, learning_rate):\n",
        "        x, y_true = inputs, outputs\n",
        "        A_1, A_2 = self.predict(inputs)\n",
        "\n",
        "        n = None # TODO (n = number of samples)\n",
        "\n",
        "        shared_coefficient = None # TODO\n",
        "        relu_gradient = np.where(A_1 > 0, 1, 0)\n",
        "\n",
        "        self.w1 = self.w1 + None # TODO\n",
        "        self.w2 = self.w2 + None # TODO\n",
        "\n",
        "    def fit(self, inputs, outputs, learning_rate, epochs=64):\n",
        "        for i in range(epochs):\n",
        "            self.update_weights_for_one_epoch(inputs, outputs, learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhnMZoohK_hP"
      },
      "source": [
        "### Training and Evaluation\n",
        "\n",
        "After designing the network structure, you can create an instance of the `Model()` class and then call the `fit` method with appropriate arguments to start training the model. It is recommended to experiment with different learning rates (such as `0.1`, `0.01`, `0.001`, etc.) and different numbers of training epochs, and compare the results on the validation samples.\n",
        "\n",
        "To assess the model's accuracy, you can use the `evaluation(model, inputs, outputs)` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNZCZNy4LCeJ"
      },
      "outputs": [],
      "source": [
        "def evaluation(model, inputs, outputs):\n",
        "  _, A_2 = model.predict(inputs)\n",
        "  prediction = (A_2 > 0.5)\n",
        "  return np.mean(prediction == outputs) * 100\n",
        "\n",
        "model = Model()\n",
        "model.fit(None, None, learning_rate = None, epochs = None) # TODO\n",
        "\n",
        "# Model evaluation\n",
        "print(f\"Your model accuracy on the given set: {round(evaluation(model, None, None), 2)}%\") # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1FMFxLLLFko"
      },
      "source": [
        "## Prediction on Test Data and Output\n",
        "\n",
        "Finally, you need to compute the model's output for the test samples. First, obtain the model's output on the test data, and then if the model predicts a higher probability that an individual has diabetes (output greater than `0.5`), classify the individual as diabetic; otherwise, classify them as non-diabetic.\n",
        "\n",
        "Therefore, in the `prediction` variable, which is a NumPy array, you will have `True` and `False` values. Note that this variable will also be evaluated by the grading system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "676HkPtxLHs_"
      },
      "outputs": [],
      "source": [
        "_, output = model.predict(None) # TODO\n",
        "prediction = None # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zfxesfSLKmy"
      },
      "source": [
        "## Assignment Grading Procedure\n",
        "\n",
        "The accuracy of your model on the test data, specifically the `prediction` variable, will also be evaluated, with a minimum acceptable accuracy of **65%**.\n",
        "\n",
        "Additionally, the `test_data` DataFrame will be checked to ensure the correctness of your data normalization process.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
